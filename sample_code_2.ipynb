{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives:\n",
    "\n",
    "    1. Writing parallel web-scraping code from the serial version of code* \n",
    "\n",
    "    2. Identifying the most-used words using mrjob (MapReduce).\n",
    "\n",
    "    3. Streaming real-time stock data and creating a SNS (Simple Notification Service) to track price fluctuation \n",
    "    using Boto3. \n",
    "\n",
    "*Baesens, B. and S. vanden Broucke (2018). Practical Web Scraping for Data Science: Best Practices and Examples with Python.  New York:  Apress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyWren  parallel  scraping  code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rVfXr2d2PPZD"
   },
   "source": [
    "**Question 1**\n",
    "\n",
    "Running time for the parallel version of web-scraping code is 32.09 seconds, while the  running time for the serial version of the web-scarping code is 447.59 seconds.\n",
    "\n",
    "**The running time for the parallelized code is around 8% of the serial version of the web-scraping code.**\n",
    "\n",
    "**Scaling through AWS Lambda:**\n",
    "The time dropped from around 80 seconds to 32 seconds when the number of partitions/batches size was increased from 10 to 40.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lQhh4N1PPN1m"
   },
   "outputs": [],
   "source": [
    "# ls_a2_q1.py\n",
    "\n",
    "import pywren\n",
    "import requests\n",
    "import dataset\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import sqlalchemy\n",
    "\n",
    "db = dataset.connect('sqlite:///books.db')\n",
    "\n",
    "base_url = 'http://books.toscrape.com/'\n",
    "\n",
    "def scrape_books(html_soup, url):\n",
    "    for book in html_soup.select('article.product_pod'):\n",
    "        book_url = book.find('h3').find('a').get('href')\n",
    "        book_url = urljoin(url, book_url)\n",
    "        path = urlparse(book_url).path\n",
    "        book_id = path.split('/')[2]\n",
    "        \n",
    "def scrape_book(html_soup, book_id):\n",
    "    main = html_soup.find(class_='product_main')\n",
    "    book = {}\n",
    "    book['book_id'] = book_id\n",
    "    book['title'] = main.find('h1').get_text(strip=True)\n",
    "    book['price'] = main.find(class_='price_color').get_text(strip=True)\n",
    "    book['stock'] = main.find(class_='availability').get_text(strip=True)\n",
    "    book['rating'] = ' '.join(main.find(class_='star-rating')\n",
    "                              .get('class')).replace('star-rating', '').strip()\n",
    "    book['img'] = html_soup.find(class_='thumbnail').find('img').get('src')\n",
    "    desc = html_soup.find(id='product_description')\n",
    "    book['description'] = ''\n",
    "    if desc:\n",
    "        book['description'] = desc.find_next_sibling('p') \\\n",
    "                                  .get_text(strip=True)\n",
    "    book_product_table = html_soup.find(\n",
    "        text='Product Information').find_next('table')\n",
    "    for row in book_product_table.find_all('tr'):\n",
    "        header = row.find('th').get_text(strip=True)\n",
    "        # Since we'll use the header as a column, cleaning it\n",
    "        # to make sure SQLite will accept it\n",
    "        header = re.sub('[^a-zA-Z]+', '_', header)\n",
    "        value = row.find('td').get_text(strip=True)\n",
    "        book[header] = value\n",
    "\n",
    "\n",
    "# Scrape the pages in the catalogue\n",
    "url = base_url\n",
    "inp = input('Do you wish to re-scrape the catalogue (y/n)? ')\n",
    "while True and inp == 'y':\n",
    "    r = requests.get(url)\n",
    "    html_soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    scrape_books(html_soup, url)\n",
    "    # Is there a next page?\n",
    "    next_a = html_soup.select('li.next > a')\n",
    "    if not next_a or not next_a[0].get('href'):\n",
    "        break\n",
    "    url = urljoin(url, next_a[0].get('href'))\n",
    "\n",
    "def do_almost_scraping(bk):\n",
    "    for val in bk:\n",
    "        book_id = val\n",
    "        book_url = base_url + 'catalogue/{}'.format(book_id)\n",
    "        r = requests.get(book_url)\n",
    "        r.encoding = 'utf-8'\n",
    "        html_soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        scrape_book(html_soup, book_id)\n",
    "    return bk\n",
    "\n",
    "def make_groups(input_list, partitions = 1):\n",
    "    length=len(list(input_list))\n",
    "    return [input_list[i*length // partitions: (i+1)*length // partitions]\n",
    "            for i in range(partitions)]\n",
    "\n",
    "start=time.time()\n",
    "\n",
    "# Now scrape book by book, oldest first\n",
    "books = db['books'].find(order_by=['last_seen'])\n",
    "my_book_ids=[]\n",
    "for book in books:\n",
    "    my_book_ids.append(book['book_id'])\n",
    "\n",
    "\n",
    "#Setting up Pywren to scale via AWS Lambda\n",
    "wrenexec=pywren.default_executor()\n",
    "\n",
    "\n",
    "#Using executor's map function with 40 partitions/batches\n",
    "futures=wrenexec.map(do_almost_scraping, make_groups(my_book_ids,40))\n",
    "\n",
    "#collecting all the results\n",
    "results=pywren.get_all_results(futures)\n",
    "\n",
    "for val in results:\n",
    "    db['books'].upsert({'book_id': val, 'last_seen': datetime.now()\n",
    "                        }, ['book_id'])\n",
    "\n",
    "end=time.time()\n",
    "print(\"Time (in seconds): %f\" % (end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bLYtkFCLYSLr"
   },
   "source": [
    "**Question 1(b)**\n",
    "\n",
    "The relational database used in the scraping solution provides meaning to the data stored in its two tables: ‘books’, and ‘book_info’. ‘books’ table stores preliminary information about books like their id and last seen, while ‘book_info’ table stores detailed information about each book. For example, it has information on ‘title’, ‘price’, etc. Maintaining two different tables  when there’s meaningful  relationship between them makes querying a lot faster. It also provides backup to your data in some sense. One can easily write queries combining information from the two tables or write independent queries. Overall it saves cost and effort in procuring results from the large database.\n",
    "\n",
    "Strength and limitations of moving over to large-scale database solutions on AWS:\n",
    "Though relational databases are efficient over other databases, however, they might not render queries quickly if the volume of data is huge. For example, if my research group was facing dataset as huge as Amazon Customer Review/Product Reviews dataset which has reviews on millions of products by millions of people, I would definitely consider large-scale solution and host data on AWS servers. Some of the main benefits include: flexibility to choose between different storage options, scalability as and when needed, and security of the data. Moreover, if one has a good sense of the project requirements then AWS services could prove inexpensive when compared to other solutions in the market or increasing storage capacity/computation power on one’s own end.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying the top 10 words using mrjob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oudLzbFWNzY1"
   },
   "source": [
    "Question **2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Swmc1315SyfJ"
   },
   "source": [
    "**Top ten words used in the book  description**\n",
    "\n",
    "\n",
    "Output:\n",
    "\n",
    "null\t[[\"the\", 5368], [\"and\", 3764], [\"of\", 3190], [\"a\", 3078], [\"to\", 2591], [\"in\", 1794], [\"is\", 1328], [\"her\", 1054], [\"that\", 917], [\"for\", 854]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S2wPXS29R7jD"
   },
   "outputs": [],
   "source": [
    "# *To write all of the book descriptions from books.db to a text file\n",
    "sqlite3 books.db \"SELECT description FROM book_info;\" > db_text_q2.txt\n",
    "\n",
    "# To find the top ten words from db_text_q2 text file using ls_a2_q2.py program\n",
    "# Code to run it from the command line\n",
    "python ls_a2_q2.py db_text_q2.txt -q\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GBfZR8o2J_le"
   },
   "outputs": [],
   "source": [
    "# ls_a2_q2.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "\n",
    "class MRToptenWords(MRJob):\n",
    "\n",
    "    def mapper_get_words(self, _, line):\n",
    "        # yield each word in the line\n",
    "        for word in WORD_RE.findall(line):\n",
    "            yield (word.lower(), 1)\n",
    "\n",
    "    def combiner_count_words(self, word, counts):\n",
    "        '''\n",
    "        Combiner to sum the words we have seen so far\n",
    "        \n",
    "        Inputs: 'word' (str) and 'counts' (int)\n",
    "        \n",
    "        Output: Tuple for each word with its total count\n",
    "        '''\n",
    "        # sum the words we've seen so far\n",
    "        yield (word, sum(counts))\n",
    "\n",
    "    def reducer_count_words(self, word, counts):\n",
    "        '''\n",
    "        Reducer to sum the words we have seen so far\n",
    "        \n",
    "        Inputs: 'word' (str) and 'counts' (int)\n",
    "        \n",
    "        Output: 'None' (no key is associated with the word-count tuple),\n",
    "                (total count,word) tuple for each word\n",
    "        '''\n",
    "        yield None, (sum(counts), word)\n",
    "\n",
    " \n",
    "    def reducer_find_max_word(self, word, word_count_pairs):\n",
    "        '''\n",
    "        Reducer function to find the word with maximum count\n",
    "        \n",
    "        Inputs: 'word' (str) and 'word_count_pairs' (tuple)\n",
    "        \n",
    "        Output: Top 10 most frequently used words with their total count\n",
    "        '''\n",
    "        yield word, Counter(dict((wrd,count) for count, wrd in word_count_pairs)).most_common(10)\n",
    "\n",
    "    def steps(self):\n",
    "        '''\n",
    "        Steps function to execute all the steps in the mrjob process\n",
    "        \n",
    "        Output: List of the  top 10 most frequently used words along with their total count\n",
    "        '''\n",
    "        \n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_words,\n",
    "                   combiner=self.combiner_count_words,\n",
    "                   reducer=self.reducer_count_words),\n",
    "            MRStep(reducer=self.reducer_find_max_word)\n",
    "        ]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRToptenWords.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Stock Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "saSbJeiaKjKm"
   },
   "source": [
    "Question **3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Script to feed data into a Kinesis stream (a producer)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LnFzrhUILFAI"
   },
   "outputs": [],
   "source": [
    "# producer.py\n",
    "\n",
    "## feed data into kinesis\n",
    "\n",
    "import json\n",
    "import boto3\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "kinesis = boto3.client('kinesis')\n",
    "\n",
    "def getReferrer():\n",
    "    data = {}\n",
    "    now = datetime.datetime.now()\n",
    "    str_now = now.isoformat()\n",
    "    data['EVENT_TIME'] = str_now\n",
    "    data['TICKER'] = 'AAPL'\n",
    "    price = random.random() * 100\n",
    "    data['PRICE'] = round(price, 2)\n",
    "    return data\n",
    "\n",
    "while True:\n",
    "        data = json.dumps(getReferrer())\n",
    "        print(data)\n",
    "        kinesis.put_record(\n",
    "                StreamName=\"ls_q3_stream\",\n",
    "                Data=data,\n",
    "                PartitionKey=\"partitionkey\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating a topic for SNS (Simple Notification Service)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xOk8ePMHQTqD"
   },
   "outputs": [],
   "source": [
    "# ls_2_q3_topic.py\n",
    "import boto3\n",
    "\n",
    "sns=boto3.client('sns', region_name='us-east-1')\n",
    "\n",
    "#Creating the topic for price alert\n",
    "topic_arn = sns.create_topic(Name='ls_q3_price_alert')['TopicArn']\n",
    "\n",
    "response=sns.subscribe(TopicArn=topic_arn, Protocol='email', Endpoint='nipun@uchicago.edu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To read the data from the Kinesis stream  (a consumer)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VjYwMtfdKmnx"
   },
   "outputs": [],
   "source": [
    "# consumer.py\n",
    "\n",
    "import boto3\n",
    "import time\n",
    "import json\n",
    "kinesis = boto3.client(\"kinesis\")\n",
    "shard_id = \"shardId-000000000000\"\n",
    "pre_shard_it = kinesis.get_shard_iterator(StreamName=\"ls_q3_stream\", ShardId=shard_id, ShardIteratorType=\"LATEST\")\n",
    "shard_it = pre_shard_it[\"ShardIterator\"]\n",
    "\n",
    "import boto3\n",
    "sns=boto3.client('sns', region_name='us-east-1')\n",
    "topic_arn = sns.create_topic(Name='ls_q3_price_alert')['TopicArn']\n",
    "\n",
    "while 1==1:\n",
    "     out = kinesis.get_records(ShardIterator=shard_it, Limit=1)\n",
    "     shard_it = out[\"NextShardIterator\"]\n",
    "     x = out['Records'][0]['Data'].decode('ASCII')\n",
    "     y=json.loads(x)\n",
    "     if y['PRICE'] < 3:\n",
    "         sns.publish(TopicArn = topic_arn, Subject = \"Price went below 3!\", Message = \"Price {price} below 3 at time {tim$\n",
    "         break\n",
    "     time.sleep(1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6QBIJyWPQ1qf"
   },
   "outputs": [],
   "source": [
    "# create_ls_q_3_stream.py\n",
    "\n",
    "# Simple script to create a kinesis stream\n",
    "import boto3\n",
    "\n",
    "client = boto3.client('kinesis')\n",
    "response = client.create_stream(\n",
    "   StreamName='ls_q3_stream',\n",
    "   ShardCount=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FKxeFrrrLTze"
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "\n",
    "session = boto3.Session()\n",
    "\n",
    "kinesis = session.client('kinesis', region_name='us-east-1')\n",
    "ec2 = session.resource('ec2', region_name='us-east-1')\n",
    "ec2_client = session.client('ec2', region_name='us-east-1')\n",
    "\n",
    "instances = ec2.create_instances(ImageId='ami-0915e09cc7ceee3ab',\n",
    "                                 MinCount=1,\n",
    "                                 MaxCount=2,\n",
    "                                 InstanceType='t2.micro',\n",
    "                                 KeyName='Macs_acc_nipun',\n",
    "                                )\n",
    "\n",
    "# Wait until EC2 instances are running before moving on\n",
    "waiter = ec2_client.get_waiter('instance_running')\n",
    "waiter.wait(InstanceIds=[instance.id for instance in instances])\n",
    "\n",
    "i_id = [instance.id for instance in instances]\n",
    "\n",
    "instance_dns = [instance.public_dns_name\n",
    "                 for instance in ec2.instances.all()\n",
    "                 if instance.state['Name'] == 'running'\n",
    "               ]\n",
    "\n",
    "code = ['producer.py', 'consumer.py']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m_RSfIVXoiAs"
   },
   "outputs": [],
   "source": [
    "import paramiko\n",
    "from scp import SCPClient\n",
    "ssh_producer, ssh_consumer = paramiko.SSHClient(), paramiko.SSHClient()\n",
    "\n",
    "# Initialization of SSH tunnels takes a bit of time; otherwise get connection error on first attempt\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "# Install boto3 on each EC2 instance and Copy our producer/consumer code onto producer/consumer EC2 instances\n",
    "\n",
    "instance = 0\n",
    "\n",
    "stdin, stdout, stderr = [[None, None] for i in range(3)]\n",
    "\n",
    "for ssh in [ssh_producer, ssh_consumer]:\n",
    "    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "    ssh.connect(instance_dns[instance],\n",
    "                username = 'ec2-user',\n",
    "                key_filename='MACS_acc_nipun.pem')\n",
    "\n",
    "    with SCPClient(ssh.get_transport()) as scp:\n",
    "        scp.put(code[instance])\n",
    "\n",
    "    '''\n",
    "    if instance == 0:\n",
    "        stdin[instance], stdout[instance], stderr[instance] = \\\n",
    "            ssh.exec_command(\"sudo pip install boto3 testdata\")\n",
    "    else:\n",
    "    '''\n",
    "    stdin[instance], stdout[instance], stderr[instance] = \\\n",
    "            ssh.exec_command(\"sudo pip install boto3\")\n",
    "\n",
    "    instance += 1\n",
    "\n",
    "\n",
    "# Block until Producer has installed boto3 and testdata, then start running Producer script:\n",
    "producer_exit_status = stdout[0].channel.recv_exit_status()\n",
    "if producer_exit_status == 0:\n",
    "    ssh_producer.exec_command(\"python %s\" % code[0])\n",
    "    print(\"Producer Instance is Running producer.py\\n.........................................\")\n",
    "else:\n",
    "    print(\"Error\", producer_exit_status)\n",
    "\n",
    "# Close ssh and show connection instructions for manual access to Consumer Instance\n",
    "ssh_consumer.close; ssh_producer.close()\n",
    "\n",
    "print(\"Connect to Consumer Instance by running: ssh -i \\\"MACS_acc_nipun.pem\\\" ec2-user@%s\" % instance_dns[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Terminating EC2 instances and deleting the Kinesis stream**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v9rRHQzlMbnB"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Terminate EC2 Instances:\n",
    "ec2_client.terminate_instances(InstanceIds=[instance.id for instance in instances])\n",
    "\n",
    "# Confirm that EC2 instances were terminated:\n",
    "waiter = ec2_client.get_waiter('instance_terminated')\n",
    "waiter.wait(InstanceIds=[instance.id for instance in instances])\n",
    "print(\"EC2 Instances Successfully Terminated\")\n",
    "\n",
    "# Delete Kinesis Stream (if it currently exists):\n",
    "try:\n",
    "    response = kinesis.delete_stream(StreamName='ls_q3_stream')\n",
    "except kinesis.exceptions.ResourceNotFoundException:\n",
    "    pass\n",
    "\n",
    "# Confirm that Kinesis Stream was deleted:\n",
    "waiter = kinesis.get_waiter('stream_not_exists')\n",
    "waiter.wait(StreamName='ls_q3_stream')\n",
    "print(\"Kinesis Stream Successfully Deleted\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gwTdk8LQO-Xz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "ls_a2_nipun.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
